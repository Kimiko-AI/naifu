trainer:
  model_path: ""
  resolution: 1024
  batch_size: 1
  checkpoint_dir: "checkpoints/lumina_sft_gradient"
  save_every: 1000
  grad_clip: 1.0
  max_steps: 10000
  seed: 42

dataset:
  index_file: "path/to/gradient/dataset.arrow"
  random_flip: True
  multireso: False
  num_workers: 4

optimizer:
  name: "torch.optim.AdamW"
  params:
    lr: 1.0e-5
    weight_decay: 0.01

scheduler:
  name: "torch.optim.lr_scheduler.CosineAnnealingLR"
  params:
    T_max: 10000

advanced:
  train_text_encoder: False
  snr_type: "log-logistic"
  no_shift: False
